\documentclass[12pt]{article}
\usepackage{geometry}
\geometry{a4paper}
\usepackage[colorlinks, linkcolor=blue, citecolor=blue, urlcolor=blue]{hyperref}
\usepackage[automake]{glossaries-extra}
\usepackage{appendix}
\usepackage{graphicx} % Needed for including images
\usepackage{mdframed} % For creating framed boxes
\usepackage[backend=biber, style=ieee]{biblatex} % Adding biblatex with IEEE style
\usepackage{minted}
\usepackage{array}
\addbibresource{Reference.bib} % Specify the bibliography file, here 'references.bib'


\makeglossaries % Initialize the glossary system

% Define some terms


\newglossaryentry{davinci}{
    name= Da Vinci Platform,
    description={ The da Vinci system is composed of three primary components: the patient-side cart, the surgeon console, and the vision cart. Notably, the da Vinci SP and da Vinci 5 systems stand out for offering seven degree-of-freedom (DOF) through their wristed instruments, whereas the da Vinci Xi system utilizes five DOF. The surgeon console, positioned a short distance away from the operating table, enables the surgeon to manipulate the surgical instruments and camera. The purpose of the vision cart is to provide reliable and intuitive control over the instruments, offer six DOF in terms of dexterity, and deliver immersive three-dimensional (3D) visualization. }
}


\newabbreviation{ots}{OTS}{
      Optical Tracking System}
\newabbreviation{emts}{EMTS}{
      Electromagnetic Tracking System}      
\newabbreviation{rmis}{RMIS}{
      Robot-assisted minimally invasive surgery}
\newabbreviation{pnp}{PnP}{
      Perspective-n-Point}
\newabbreviation{6dof}{6DoF}{
      Six Degrees of Freedom}
\newabbreviation{p3p}{P3P}{
      Perspective-3-Point}
\newabbreviation{epnp}{EPnP}{
      Efficient Perspective-n-Point
}
\newacronym{sift}{SIFT}{Scale Invariant Feature Transform}
\newacronym{surf}{SURF}{Speeded Up Robust Feature}
\newacronym{icp}{ICP}{Iterative Nearest Point}


% Define an acronym




\begin{document}

\begin{titlepage}
      \centering

      \includegraphics[width=0.8\textwidth]{Imperial_College_London_new_logo.png} % Increased width
      \vspace*{1cm}

      \Large
      SURG70006 Group Project

      \large
      2024/10

      \vspace{0.5cm}
      \Huge
      \textbf{Project 19 \\ Surgical Robot Instrument Pose Estimation }

      \vspace{1.3cm}


      % Framed box for student information
      \begin{mdframed}
            \normalsize % Smaller text size within the box
            \textbf{Group Number:} Group 8\\[20pt] % Name on the same line, add vertical space
            \textbf{Group Members:} Jie Li, Jinling Qiu, Leen AIShekh, Yanrui Liu, Yulin Huang\\[20pt] % ID on the same line, add vertical space
            \textbf{Supervisor Name:} Dr Stamatia (Matina) Giannarou % Supervisor on the same line
      \end{mdframed}

      \vspace{2cm} % Adjust space as necessary
      \Large
      \textbf{DEPARTMENT OF}\\
      \vspace{0.1cm} % Adjust line spacing
      \textbf{Surgery and Cancer}

      \vspace{4cm} % Large space as required
      \large
      Imperial College London\\
      


\end{titlepage}

\newpage
\tableofcontents

\newpage

% Introduction & Background
\section{Introduction}
\subsection{Minimally invasive surgery and Robotic surgery(TBS)}



\subsection{Engineering Background}
\subsubsection{DoF of da Vinci Surgical Robot}
The da Vinci surgical robot operates with \gls{6dof}. These \gls{6dof} refer to the robot's ability to move and rotate in three-dimensional space, including three translational movements (up/down, left/right, forward/backward) and three rotational movements (pitch, yaw, and roll)\cite{app9030546}. This range of motion allows the da Vinci system to replicate the complex dexterity of a surgeon's hand for precise control over surgical instruments in confined spaces.

\subsubsection{Difficulties of Surgical Tools Tracking}
Pose estimation refers to finding the transformation (translation and rotation) that relates the object (or camera) coordinates in 3D space to its projection on a 2D image. The pose estimation of surgical tools has emerged as a critical job in \gls{rmis}. The majority of robots in \gls{rmis} are driven by cables, resulting in kinematic input that is not always precise, since the kinematic data describes the positions of the motor rather than the real position of the joints connected to the motor via a cable\cite{10160287}. 

\subsubsection{Limitations of OTS and EMTS}
\gls{ots} and \gls{emts} are well-established methods for tracking in medical applications. \gls{ots} offers high accuracy but requires a clear line-of-sight, making it prone to errors when obstructed. \gls{emts}, while effective without line-of-sight, suffers from interference caused by metal objects and electronic devices in the operating room, leading to reduced accuracy\cite{8822749}.

\subsubsection{Vision-based Methods}
\begin{enumerate}
      \item \textbf{PnP Problem and Solvers}
      \\\gls{pnp} problem was proposed by Fischler in 1980s, which aims at estimating the position and orientation of a calibrated camera based on known 3D-to-2D point correspondences between a 3D model and their image projections\cite{Fischler1981RandomSC}. The \gls{pnp} is a fundamental problem of many computer vision applications, among which self-motion estimation for robots is a problem of interest.
      \gls{p3p} and \gls{epnp} are two common solutions to the \gls{pnp} problem in camera pose estimation\cite{Lu_2018}. \gls{p3p} calculates the camera pose using a 3D-2D correspondences, which provides up to four possible solutions and can be disambiguated by adding a fourth point. It is suitable for minimal data scenarios. In contrast, \gls{epnp} handles large datasets more efficiently by representing n 3D points as a weighted sum of four virtual control points, reducing computational complexity while maintaining accuracy\cite{10.1007/s11263-008-0152-6}.
      
      \item \textbf{Marker-based and Marker-less Methods}
      \\In gls{rmis}, a marker-based method involves placing artificial markers on surgical instruments to aid vision-based instrument tracking\cite{villani2021development}. These markers can often be easily recognised in complex surgical environments. However, if the markers are obscured, damaged, or obscured by blood coverage, this may result in detection failure\cite{ma2021comprehensive}. In addition, markers on the surface of surgical instruments must meet sterility requirement\cite{xu2023graph}. To address these issues, marker-less methods have been gradually proposed\cite{reilink20133d}. marker-less methods do not rely on artificial markers in endoscopic procedures, but rather on natural features of the surgical instruments for gesture estimation. This method does not require additional marking process for the instruments and is able to adapt to various environmental changes with higher flexibility. However, the current marker-less method still faces some challenges, such as being susceptible to interference from lighting conditions, blood occlusion, and instrument reflections, and may not perform as consistently as the marker-based method in complex scenes\cite{hein2021towards}.
\end{enumerate}

\subsection{Clinical Aspects(TBD)}

\subsection{Problem Statement}
\gls{rmis} has come significantly in the last decade due to advances in surgical robotics such as artificial intelligence and the \gls{davinci}. Pose estimation of surgical instruments has become an important task in \gls{rmis}. 
Nowadays there are many external devices like depth camera, electromagnetic trackers etc. available for space estimation in surgical instruments but they are not practical in in vivo surgeries because of space and hardware constraints\cite{enhancedmarker}. There are some vision-based methods that use external markers to track the instruments. However, these methods have major limitations; the markers must always be visible in the camera's field of view and are sensitive to background changes and occlusions\cite{10160287}. In this case, a vision-based markerless instrument tracking method that does not require any modifications to the hardware setup or external markers is necessary. The main aim of this project is to develop a deep learning based markerless \gls{6dof} surgical instrument pose estimation system. The system will be designed to provide highly accurate surgical instrument \gls{6dof} estimation without relying on external markers or complex hardware.

\begin{figure}[H]
            \centering
            \includegraphics[width=0.8\textwidth]{6Dof.png}
            \caption{\gls{6dof} surgical instrument pose estimation with (left) and without occlusion (right). \cite{surgripe2024}}
      \end{figure}


\section{Related Work}
\subsection{Traditional non-learning methods}
Traditional non-learning pose estimation methods are based on geometric modelling, algebraic techniques, and computer vision approaches\cite{fan2024reinforcement}. Non-learning methods differ from deep learning methods, which require a large amount of data as input. Typically, the core of traditional non-deep learning methods is featuring extraction, which is used to obtain a unique representation of an object by identifying edges, keypoints, or regional features\cite{fan2024reinforcement}. In the early days, the main methods for feature recognition include \gls{sift}\cite{lakshmi2017image}, which is a classical local feature descriptor that helps machines to identify and match feature points in different images, and to find key points in different scale spaces. Another is the \gld{surf}\cite{wijesinghe2010speed}, which is an improved variant of \gls{sift} that improves the performance of feature extraction by optimising the process of feature detection and description.
After feature extraction is completed, the target model needs to be parameterised. The \gls{pnp}\cite{[yun2017object,lu2018review} algorithm is a commonly used algorithm for model parameterisation, which uses a set of 3D points in the scene and their 2D projections in the image to estimate translation and rotation parameters. Another method is the \gls{icp} algorithm\cite{bellekens2014survey}, which computes the pose relationship between two-point clouds by minimising the distance between corresponding points.
Non-learning methods have better interpretability and can achieve more accurate results while saving computational resources. However, such methods are less robust when dealing with complex scenes and lighting changes and have certain limitations\cite{bellekens2014survey}.

\subsection{Deep Leaning Method}
In recent years, with the development of deep learning, it has been gradually applied to the field of surgical instrument pose estimation. Different from the traditional way that relies on geometric models and manual feature extraction, deep learning methods can infer the complex relationship between points and points from a large amount of data \cite{ bellekens2014survey}, and according to the facts, deep learning methods are more capable of handling complex scenes with lighting changes\cite{fan2024reinforcement}. Through the concern classification of the model, it can be divided into Holistic method and Intermediate representation method.
\subsection{2.1.1 Holistic}
The Holistic method extracts estimated surgical instrument poses by modelling global features of the entire scene\cite{fan2024reinforcement}. This approach does not rely on local detail features, but rather extracts pose information from global features making the Holistic method highly robust to complex scene variations. In 2015, Alex Kendall and his team proposed PoseNet, a deep learning method that directly regresses camera pose from monocular RGB images, enabling end-to-end position and orientation end-to-end estimation\cite{kendall2015posenet}. In 2020, Yannick Bukschat et al. proposed EfficientPose, an end-to-end 6D multi-target pose estimation method. The model is capable of simultaneously detecting the 2D bounding boxes of multiple targets in a monocular image and regressing their complete 6D poses in 3D space\cite{bukschat2020efficientpose}. In 2022, Bo Chen and colleagues developed the ROPE framework, which introduces a new occlusion enhancement technique and a multi-precision supervised mechanism, aiming to learn deep features that are robust to occluded environments, thus improving the accuracy of pose estimation in object-occluded scenes\cite{chen2022occlusion}.
The Holistic method is able to capture the overall features of an object directly from the whole image, with a low dependence on feature points, without the need for precise positioning of feature points or additional feature extraction steps, which makes the model structure more concise\cite{chen2022occlusion}. However, the Holistic method is less accurate in dealing with local details, and when surgical instruments are occluded, it is difficult to recover the occluded instrument information from the overall features\cite{watson2014nature}.

\subsection{2.1.2 Intermediate Representations}
The Intermediate Representation method decomposes the complex pose estimation task into multiple more manageable subtasks by introducing a finer-grained intermediate description of the target. By extracting local features, the method effectively solves the problem that it is difficult to accurately estimate the pose of surgical instruments when they are occluded\cite{song2020hybridpose}.
In 2017, Yu Xiang and his team proposed PoseCNN for pose estimation in complex scenes. The method decomposes the pose estimation task into multiple components that deal with 3D translations and rotations of images separately. In addition, PoseCNN introduces a novel loss function that allows the network to better handle objects with symmetry\cite{xiang2017posecnn}.
Subsequently, in 2019, Sida Peng and his team proposed PVNet\cite{peng2019pvnet}. this approach uses a pixel-level voting network that significantly improves pose estimation accuracy in occluded and truncated scenes by predicting vectors from each pixel to a key point, combined with a RANSAC-based voting mechanism.
In 2020, Masakazu Yoshimura and his team developed a deep learning model based on an improved SSD-6D architecture\cite{yoshimura2020single}. The model utilises a manually generated dataset of single-frame endoscopic images combined with data enhancement techniques to effectively address occlusion and perspective distortion problems common in surgical environments.
In 2022, Mitchell Doughty and his team proposed HMD-EgoPose\cite{yoshimura2020single}. The method uses the EfficientDet-D0 network for multi-scale feature extraction and combines rotational, translational, and hand sub-networks to achieve 6-degree-of-freedom markerless pose estimation in monocular RGB images.
In 2024, Jihun Park and his team introduced a new occlusion-aware loss function based on the YOLOv8 model, which dramatically improved the accuracy of precise detection and pose estimation of key points of surgical instruments in complex occlusion environments\cite{park2024towards}. The research team trained the model on a real surgical dataset, which significantly improved its robustness in real surgical scenarios.
The Intermediate representation method makes the task much less difficult by decomposing the complex pose estimation task into multiple, more manageable subtasks. At the same time, Intermediate Representation models local features so that the model can still maintain high stability in complex scenes. However, this method requires high accuracy in data labelling, and the accumulation of errors may affect the accuracy of the final results due to the inclusion of multiple intermediate steps\cite{xu2023graph,allan20183}.


\section{Proposed Methodlogy}


\section{Goals and Objectives}
\subsection{Objectives of the Project}
This project aims to leverage state-of-the-art deep learning models for pose estimation to accurately determine the rotation and position of surgical tools present in the surgical environment during \gls{rmis}. The detailed objectives of the project are as follows:

\begin{enumerate}

\item \textbf{Dataset Analysis}
\\The first objective is to analyze the datasets which include high-quality images captured by the Da Vinci Si endoscopic stereo camera and accurate and consistent ground truth data obtained from the Hamlyn Centre.

\item \textbf{Model Development}
\\Based on the available datasets, we will develop deep learning-based models to detect and estimate the pose of surgical instruments in \gls{rmis}.

\item \textbf{Robust Pose Estimation}
\\We also need to devise novel approaches to ensure accurate and robust pose estimation in the presence of challenges such as partial tool visibility, occlusions, and other variations encountered during surgery. 

\item \textbf{Performance Evaluation}

Finally, the project will evaluate and validate the performance of the applied models under various degrees of occlusion, ensuring their reliability in practical surgical scenarios.

\end{enumerate}


\subsection{Hardware and Software Requirements(TBD)}



\section{Risk Assessment}
\begin{table}[H]
      \centering
      \renewcommand{\arraystretch}{1.5} 
      \resizebox{\textwidth}{!}{
      \begin{tabular}{| >{\centering\arraybackslash}m{4cm} | >{\centering\arraybackslash}m{5cm} | >{\centering\arraybackslash}m{2.5cm} | >{\centering\arraybackslash}m{2.5cm} |}
      \hline
      \normalsize\textbf{Risk} & \normalsize\textbf{Contingencies} & \normalsize\textbf{Likelihood} & \normalsize\textbf{Impact} \\ 
      \hline
      \normalsize Pose estimation unstable in complex scenes (e.g., occlusion, dynamic background) & \normalsize Optimizing the dataset with more occlusion scene data & \normalsize High & \normalsize Very High \\ 
      \hline
      \normalsize Low Frame Rates & \normalsize Optimizing model efficiency, invest in high-quality hardware & \normalsize High & \normalsize Medium \\ 
      \hline
      \normalsize Model computation time is too long for real-time simulation & \normalsize Optimizing model efficiency and using hardware acceleration & \normalsize High & \normalsize Medium \\ 
      \hline
      \normalsize Reliance on specific deep learning frameworks, leading to migration difficulties & \normalsize Reduced binding to specific frameworks & \normalsize Medium & \normalsize High \\ 
      \hline
      \normalsize Data damage or lost & \normalsize Using GitHub or external storage devices to make backup & \normalsize Medium & \normalsize Very High \\ 
      \hline
      \normalsize Data Privacy & \normalsize Implementing robust data encryption and comply with data protection laws such as GDPR & \normalsize Medium & \normalsize Very High \\ 
      \hline
      \normalsize Ethical and Regulatory & \normalsize Consulting with regulatory experts and following related regulations & \normalsize Very Low & \normalsize High \\ 
      \hline
      \normalsize Timeline delay & \normalsize Making a detailed timeline and a thorough monitoring plan & \normalsize High & \normalsize High \\ 
      \hline
      \end{tabular}
      }
      \caption{Risk Assessment Table}
\end{table}

\section{Project Timeline}

\section{Project Management}
\subsection{Progress Monitoring}
For code part, we use GitHub for monitoring and progress management. We use GitHub's version control to branch the code (each team member manages a branch independently) to ensure smooth team collaboration. We also manually record project logs(such as meeting minutes and group activities) and combine them with timelines to ensure the project run smoothly.

% References (The bibliography will be printed here)
\printbibliography
\printglossaries
\label{sec:glossary}
\end{document}
